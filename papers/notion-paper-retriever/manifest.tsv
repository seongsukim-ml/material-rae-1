page_id	year	short_name	title	url	status	notion_url
11614e7b-ab87-4d1a-ab38-daf9ecf51560	2026	Scale-RAE	Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders	https://arxiv.org/abs/2601.16208	To Read	https://www.notion.so/Scaling-Text-to-Image-Diffusion-Transformers-with-Representation-Autoencoders-11614e7bab874d1aab38daf9ecf51560
14efae63-0980-4276-b4a5-a59d14c631b3	2024	Universal Matter Reps	Universally Converging Representations of Matter Across Scientific Foundation Models	https://arxiv.org/abs/2512.03750	To Read	https://www.notion.so/Universally-Converging-Representations-of-Matter-Across-Scientific-Foundation-Models-14efae6309804276b4a5a59d14c631b3
18895db7-a1c4-458f-8eea-36a065747eb6	2024	HELM	Learning from the electronic structure of molecules across the periodic table	https://arxiv.org/abs/2510.00224	To Read	https://www.notion.so/Learning-from-the-electronic-structure-of-molecules-across-the-periodic-table-18895db7a1c4458f8eea36a065747eb6
21c2ba4c-dbcc-4baa-81d1-f1e5814ceefb	2022	ViT Analysis	How Do Vision Transformers Work?	https://arxiv.org/abs/2202.06709	To Read	https://www.notion.so/How-Do-Vision-Transformers-Work-21c2ba4cdbcc4baa81d1f1e5814ceefb
243dec6a-2274-48cd-b67b-d68e2faafae1	2025	VA-VAE	Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models	https://arxiv.org/abs/2501.01423	To Read	https://www.notion.so/Reconstruction-vs-Generation-Taming-Optimization-Dilemma-in-Latent-Diffusion-Models-243dec6a227448cdb67bd68e2faafae1
2fd56c39-27f8-441d-ac06-218932b6a576	2025	REG	Representation Entanglement for Generation: Training Diffusion Transformers Is Much Easier Than You Think	https://arxiv.org/abs/2507.01467	To Read	https://www.notion.so/Representation-Entanglement-for-Generation-Training-Diffusion-Transformers-Is-Much-Easier-Than-You--2fd56c3927f8441dac06218932b6a576
478c3c9e-202f-4cef-b7ca-488f78b32131	2025	REPA	Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think	https://arxiv.org/abs/2410.06940	To Read	https://www.notion.so/Representation-Alignment-for-Generation-Training-Diffusion-Transformers-Is-Easier-Than-You-Think-478c3c9e202f4cefb7ca488f78b32131
4d0fa9e4-ac2f-4c7f-aedb-49bc634384c0	2024	Platonic MLIP	Platonic representation of foundation machine learning interatomic potentials	https://arxiv.org/abs/2512.05349	To Read	https://www.notion.so/Platonic-representation-of-foundation-machine-learning-interatomic-potentials-4d0fa9e4ac2f4c7faedb49bc634384c0
4efdfc5b-9aa4-4334-aed0-1c2f6211e4ed	2025	Force-Conf REPA	Unifying Force Prediction and Molecular Conformation Generation Through Representation Alignment	https://openreview.net/forum?id=yzkHGHvC74	To Read	https://www.notion.so/Unifying-Force-Prediction-and-Molecular-Conformation-Generation-Through-Representation-Alignment-4efdfc5b9aa44334aed01c2f6211e4ed
55c1d45a-69bc-4c4c-b517-d2f933bdd799	2024	l-DAE	Deconstructing Denoising Diffusion Models for Self-Supervised Learning	https://arxiv.org/abs/2401.14404	To Read	https://www.notion.so/Deconstructing-Denoising-Diffusion-Models-for-Self-Supervised-Learning-55c1d45a69bc4c4cb517d2f933bdd799
58818d6e-7c7f-4298-88b0-4e5bdd1440bc	2026	RAE	RAE: A Neural Network Dimensionality Reduction Method for Nearest Neighbors Preservation in Vector Search	https://arxiv.org/abs/2509.25839	To Read	https://www.notion.so/RAE-A-Neural-Network-Dimensionality-Reduction-Method-for-Nearest-Neighbors-Preservation-in-Vector-S-58818d6e7c7f429888b04e5bdd1440bc
682d266a-70bf-45ee-a052-5bee943c4693	2022	Grokking	Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets	https://arxiv.org/abs/2201.02177	To Read	https://www.notion.so/Grokking-Generalization-Beyond-Overfitting-on-Small-Algorithmic-Datasets-682d266a70bf45eea0525bee943c4693
92133b34-86d8-4553-a1c7-aa2dddb448d7	2023	I-JEPA	Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture	https://arxiv.org/abs/2301.08243	To Read	https://www.notion.so/Self-Supervised-Learning-from-Images-with-a-Joint-Embedding-Predictive-Architecture-92133b3486d84553a1c7aa2dddb448d7
99975b9a-b392-44f5-819d-21e44eddb030	2025	iREPA	What Matters for Representation Alignment: Global Information or Spatial Structure?	https://arxiv.org/abs/2512.10794	To Read	https://www.notion.so/What-Matters-for-Representation-Alignment-Global-Information-or-Spatial-Structure-99975b9ab39244f5819d21e44eddb030
c76933f9-761b-4741-b278-c4cfcbefc3e6	2021	DINO	Emerging Properties in Self-Supervised Vision Transformers	https://arxiv.org/abs/2104.14294	To Read	https://www.notion.so/Emerging-Properties-in-Self-Supervised-Vision-Transformers-c76933f9761b4741b278c4cfcbefc3e6
df8c55ad-af52-44cd-ba45-cb7868194358	2024	UMLIP Comparison	Comparing the latent features of universal machine-learning interatomic potentials	https://arxiv.org/abs/2512.05717	To Read	https://www.notion.so/Comparing-the-latent-features-of-universal-machine-learning-interatomic-potentials-df8c55adaf5244cdba45cb7868194358
f02dbce9-2777-43d8-ab25-2a8ee90babfa	2024	DINOv2	DINOv2: Learning Robust Visual Features without Supervision	https://arxiv.org/abs/2304.07193	To Read	https://www.notion.so/DINOv2-Learning-Robust-Visual-Features-without-Supervision-f02dbce9277743d8ab252a8ee90babfa
fa04c8d8-defd-4b07-a6b8-8f90b714eb76	2022	EDM	Elucidating the Design Space of Diffusion-Based Generative Models	https://arxiv.org/abs/2206.00364	To Read	https://www.notion.so/Elucidating-the-Design-Space-of-Diffusion-Based-Generative-Models-fa04c8d8defd4b07a6b88f90b714eb76
